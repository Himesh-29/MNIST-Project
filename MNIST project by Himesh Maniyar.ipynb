{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing the library required in this project__\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading the MNIST(â€œModified National Institute of Standards and Technology\") dataset from tensorflow.keras library in mnist variable__\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Here x_train and x_test contain the image data in 28x28 and y_train and y_test is the number attached to that image data\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Visualizing our x_train data which is in the form of image using matplotlib.pyplot__\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[10],cmap=plt.cm.binary) #In x_train at index 10, we have element 3 in the position which we visualized through pyplot library\n",
    "#cmap=The Colormap instance or registered colormap name used to map scalar data to colors(Taken from matplotlib documentation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#Printing the corresponding value to the image shown by x_train[10] by y_train[10]\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalizing the big data which we have in x_train and x_test to manipulate it easily and to make the process fast__\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  42 118 219 166 118 118   6\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 103 242 254 254 254 254 254  66\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  18 232 254 254 254 254 254 238\n",
      "   70   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 104 244 254 224 254 254 254\n",
      "  141   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 207 254 210 254 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  84 206 254 254 254 254\n",
      "   41   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  24 209 254 254 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  91 137 253 254 254 254\n",
      "  112   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  40 214 250 254 254 254 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  81 247 254 254 254 254 254 254\n",
      "  146   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 110 246 254 254 254 254 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  73  89  89  93 240 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 128 254\n",
      "  219  31   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7 254 254\n",
      "  214  28   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 138 254 254\n",
      "  116   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  19 177  90   0   0   0   0   0  25 240 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 164 254 215  63  36   0  51  89 206 254 254 139\n",
      "    8   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  57 197 254 254 222 180 241 254 254 253 213  11\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 140 105 254 254 254 254 254 254 236   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7 117 117 165 254 254 239  50   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalization usually fades the pixels as big values are converted to small decimal values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.06923363\n",
      "  0.14712657 0.24945318 0.1796561  0.12228485 0.11590366 0.0062582\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26784526 0.3989176\n",
      "  0.31669617 0.28932012 0.27489548 0.26322332 0.24948754 0.06884022\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04680791 0.3824334\n",
      "  0.31669617 0.28932012 0.27489548 0.26322332 0.24948754 0.248242\n",
      "  0.13760186 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.17143566\n",
      "  0.30422781 0.28932012 0.24242751 0.26322332 0.24948754 0.26493053\n",
      "  0.27716945 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25809491 0.28932012 0.22727579 0.26322332 0.24948754 0.26493053\n",
      "  0.06683519 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.10473417 0.23464545 0.27489548 0.26322332 0.24948754 0.26493053\n",
      "  0.08059537 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02733733 0.22619352 0.26322332 0.24948754 0.26493053\n",
      "  0.33614168 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.11346201 0.15605062 0.27381321 0.26322332 0.24948754 0.26493053\n",
      "  0.22016297 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10401758 0.35276184\n",
      "  0.31170883 0.28932012 0.27489548 0.26322332 0.24948754 0.26493053\n",
      "  0.06683519 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2106356  0.4071597\n",
      "  0.31669617 0.28932012 0.27489548 0.26322332 0.24948754 0.26493053\n",
      "  0.28699816 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18132618\n",
      "  0.30672148 0.28932012 0.27489548 0.26322332 0.24948754 0.26493053\n",
      "  0.33614168 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09101898 0.10137595 0.09632164 0.09637704 0.23573625 0.26493053\n",
      "  0.33614168 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00103631 0.125726   0.26493053\n",
      "  0.43049724 0.74210264 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00725419 0.24948754 0.26493053\n",
      "  0.42066853 0.67028626 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.14301109 0.24948754 0.26493053\n",
      "  0.22802593 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.10878298 0.45066626 0.24969673 0.         0.         0.\n",
      "  0.         0.         0.02705664 0.24871495 0.24948754 0.26493053\n",
      "  0.06683519 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.93896892 0.64671882 0.59649775 0.1644903  0.09361582 0.\n",
      "  0.0635886  0.10137595 0.22294673 0.26322332 0.24948754 0.14498167\n",
      "  0.01572593 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.32634895 0.50158901 0.70469967 0.66318312 0.57729756 0.29671557\n",
      "  0.30048731 0.28932012 0.27489548 0.26218701 0.20921593 0.01147337\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35645919 0.29131286 0.66318312 0.66051162 0.41869864\n",
      "  0.31669617 0.28932012 0.27489548 0.2445697  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01942086 0.30548199 0.30425142 0.27198927\n",
      "  0.31669617 0.28932012 0.25866149 0.05181561 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train[10])\n",
    "\n",
    "plt.imshow(x_train[10],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Building the model for testing__\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Sequential() method will create linear stack of different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer should be flat as said above but our images are in the form of 28*28 binary data.\n",
    "#So flatten them to 1*784\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have 128 neurons in each layers which will be connected to all the input data\n",
      "Adding layers will increase the number of neural networks combination will increase the complexity of model and will yield better accuracy\n",
      "But more layers will increase the time complexity so an optimum choice should be selected\n",
      "How many layer you want to add(At least you should add 4-5 layers)? 5\n"
     ]
    }
   ],
   "source": [
    "print(\"We will have 128 neurons in each layer which will be connected to all the input data\")\n",
    "print(\"Adding layers will increase the number of neural networks combination will increase the complexity of model and will yield better accuracy\")\n",
    "print(\"But more layers will increase the time complexity so an optimum choice should be selected\")\n",
    "print(\"How many layer you want to add(At least you should add 4-5 layers)? \",end=\"\")\n",
    "x=int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Layers\n",
    "for i in range(x):\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "#We are using Dense here to assure that we have a fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Output Layer\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax)) #Here the last layers contains 10 neurons because we have digits from 0 to 9 only\n",
    "#Softmax basically yields that output for which probability is highest than for other cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training the model__\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Neural network should focus less loss before having higher accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less epochs will create more loss but takes less time than more epochs.\n",
      "At least you should take 8 epoches for better optimisation and to yield good accuracy and less loss\n",
      "How many epochs you want to run? 10\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 9s 4ms/step - loss: 0.4709 - accuracy: 0.8529\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1207 - accuracy: 0.9635\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.97 - 7s 4ms/step - loss: 0.0805 - accuracy: 0.9759\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0632 - accuracy: 0.9803\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0574 - accuracy: 0.9829\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0460 - accuracy: 0.9859: 0s - l\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0350 - accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0312 - accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0259 - accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0256 - accuracy: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d13fb5bd00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Less epochs will create more loss but takes less time than more epochs.\")\n",
    "print(\"At least you should take 8 epoches for better optimisation and to yield good accuracy and less loss\")\n",
    "print(\"How many epochs you want to run? \",end=\"\")\n",
    "x=int(input())\n",
    "model.fit(x_train, y_train, epochs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1070 - accuracy: 0.9782\n",
      "0.10699068754911423\n",
      "0.9782000184059143\n"
     ]
    }
   ],
   "source": [
    "#To see the total loss and accuracy from the model in which we have fitted our training data set\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saving the Model__\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MNIST_epic_number_reader.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('MNIST_epic_number_reader.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading the Model Back__\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('MNIST_epic_number_reader.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To check whether our model is giving proper output depending upon x_test__\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.07106857e-12 3.71532027e-09 1.14365726e-08 ... 1.00000000e+00\n",
      "  1.22490559e-11 4.54031035e-10]\n",
      " [3.34236458e-11 1.00478806e-10 1.00000000e+00 ... 4.53618618e-08\n",
      "  8.06993494e-09 2.53271471e-13]\n",
      " [4.57791450e-16 1.00000000e+00 1.45342445e-11 ... 1.39868074e-11\n",
      "  4.45217481e-08 4.75032860e-12]\n",
      " ...\n",
      " [1.72076877e-08 5.51845609e-08 1.32867328e-07 ... 1.61136015e-07\n",
      "  8.90018583e-08 3.37293216e-07]\n",
      " [1.62902753e-16 2.38337308e-19 6.78500019e-16 ... 5.22176436e-16\n",
      "  1.04454578e-09 6.38320993e-13]\n",
      " [4.20604246e-10 1.95526401e-10 2.21805018e-12 ... 2.51100772e-20\n",
      "  3.66289998e-09 3.43147386e-12]]\n"
     ]
    }
   ],
   "source": [
    "#Printing the values of prediction for x_test\n",
    "#x_test is an nd array of size 10000 x 10 ,so predictions will be of the same order\n",
    "prediction = new_model.predict(x_test) #Here prediction is holding the probabilities of various numbers in a particular dataset out of 10000\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#To see which number corresponds to the maximum value of predictions[500]\n",
    "import numpy as np\n",
    "print(np.argmax(predictions[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#To check whether our model predicted correct answer by visualizing which number is present in x_test at index 5 \n",
    "#and what is the corresponding correct value as provided in y_test\n",
    "plt.imshow(x_test[500],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(y_test[500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
